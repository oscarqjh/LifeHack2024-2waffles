{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import tqdm.notebook as tq\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_cna():\n",
    "    \"\"\"\n",
    "    Scrape Channel News Asia Topic Terrorism\n",
    "    \"\"\"   \n",
    "    # Base URL\n",
    "    cna_url=\"https://www.channelnewsasia.com/topic/terrorism\"\n",
    "    cna_base_url = 'https://www.channelnewsasia.com'\n",
    "\n",
    "    # Get webpage\n",
    "    html = requests.get(cna_url)\n",
    "\n",
    "    # Initialise bs object\n",
    "    bsobj = soup(html.content,'lxml')\n",
    "\n",
    "    # Find all headers\n",
    "    headers = bsobj.findAll(\"h6\")\n",
    "\n",
    "    # initialize the progress bar\n",
    "    # select a random color\n",
    "    colors = ['red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white', 'steelblue']\n",
    "    random_color = random.choice(colors)\n",
    "    loop = tq.tqdm(enumerate(headers), total=len(headers), \n",
    "                        leave=True, colour=random_color, unit='article')\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for _, header in loop:\n",
    "        item = {\n",
    "            'headline': header.text\n",
    "        }\n",
    "\n",
    "        # Enter news article\n",
    "        news_link = cna_base_url + header.a['href']\n",
    "        news_html = requests.get(news_link)\n",
    "\n",
    "        # Traverse news article for main body\n",
    "        article = soup(news_html.content,'lxml')\n",
    "        content = article.find('div', {'class': 'content'})\n",
    "        if content:\n",
    "            content_wrapper = content.find_all('div', {'class': 'content-wrapper'})\n",
    "\n",
    "            article_text = []\n",
    "            for content in content_wrapper:\n",
    "                # Extract text\n",
    "                main_text = content.find('div', {'class': 'text'})\n",
    "                if main_text:\n",
    "                    article_text.append(main_text.get_text())\n",
    "        \n",
    "            item['text'] = article_text\n",
    "            articles.append(item)\n",
    "        \n",
    "        loop.set_postfix(Processing=header.text)\n",
    "\n",
    "    return articles\n",
    "\n",
    "def format_articles(articles):\n",
    "    \"\"\"\n",
    "    Format scraped data\n",
    "    \"\"\"\n",
    "\n",
    "    formatted_articles = []\n",
    "\n",
    "    # Format article\n",
    "    for article in articles:\n",
    "        if 'text' not in article:\n",
    "            continue\n",
    "        body = \"\\n\".join(article['text'])\n",
    "        whole = \"\\n\".join([article['headline'], body])\n",
    "\n",
    "        formatted_articles.append(whole)\n",
    "\n",
    "    return formatted_articles\n",
    "\n",
    "def scrape_apnews():\n",
    "    \"\"\"\n",
    "    Scrape AP News Topic Terrorism\n",
    "    \"\"\"\n",
    "\n",
    "    # Base URL\n",
    "    cna_url=\"https://apnews.com/hub/terrorism\"\n",
    "\n",
    "    # Get webpage\n",
    "    html = requests.get(cna_url)\n",
    "\n",
    "    # Initialise bs object\n",
    "    bsobj = soup(html.content,'lxml')\n",
    "\n",
    "    # Find all article item in first section\n",
    "    items = bsobj.findAll(\"h3\", {'class':'PagePromo-title'})\n",
    "\n",
    "    # initialize the progress bar\n",
    "    # select a random color\n",
    "    colors = ['red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white', 'steelblue']\n",
    "    random_color = random.choice(colors)\n",
    "    loop = tq.tqdm(enumerate(items), total=len(items), \n",
    "                        leave=True, colour=random_color, unit='article')\n",
    "    \n",
    "    articles = []\n",
    "\n",
    "    for _, header in loop:\n",
    "        item = {\n",
    "            'headline': header.text\n",
    "        }\n",
    "\n",
    "        # Enter news article\n",
    "        news_link = header.a['href']\n",
    "        news_html = requests.get(news_link)\n",
    "\n",
    "        # Traverse news article for main body\n",
    "        article = soup(news_html.content,'lxml')\n",
    "        content = article.find('div', {'class': 'RichTextStoryBody'})\n",
    "        if content:\n",
    "            article_texts = content.find_all('p')\n",
    "            article_text = []\n",
    "            for text in article_texts:\n",
    "                article_text.append(text.text)\n",
    "            item['text'] = article_text\n",
    "\n",
    "        articles.append(item)\n",
    "        \n",
    "        # Update progress description\n",
    "        loop.set_postfix(Processing=header.text)\n",
    "    \n",
    "    return articles\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2039b4f0f10145559f0bf21f767634a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?article/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles = scrape_apnews()\n",
    "formatted_articles_apnews = format_articles(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_articles_apnews[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da054e187fc4fc0b83f4b3dd1fea67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?article/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles = scrape_cna()\n",
    "formatted_articles_cna = format_articles(articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_articles_cna[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "mongo_password = os.getenv(\"MONGO_PASSWORD\")\n",
    "\n",
    "# MongoDB connection setup\n",
    "client = MongoClient('mongodb+srv://tristantanjh:{mongo_password}@cluster0.igmtl9j.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0')\n",
    "db = client.articles_db\n",
    "articles_collection = db.articles\n",
    "\n",
    "def generate_article_id(title, date_published):\n",
    "    # Create a unique identifier using title and date\n",
    "    unique_string = title + date_published\n",
    "    return hashlib.md5(unique_string.encode()).hexdigest()\n",
    "\n",
    "def store_article_in_mongo(article):\n",
    "    # Check if the article already exists\n",
    "    article_id = generate_article_id(article['title'], article['timestamp_published'])\n",
    "    existing_article = articles_collection.find_one({'title': article['title']})\n",
    "    if existing_article is not None:\n",
    "        print(f\"Article already exists: {article['title']}\")\n",
    "        return False\n",
    "\n",
    "    # Insert new article into MongoDB\n",
    "    articles_collection.insert_one({\n",
    "        '_id': article_id,\n",
    "        'title': article['title'],\n",
    "        'timestamp_published': article['timestamp_published']\n",
    "    })\n",
    "    print(f\"Stored new article: {article['title']}\")\n",
    "    return True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
