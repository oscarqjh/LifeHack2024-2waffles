{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tristan\\AppData\\Local\\Temp\\ipykernel_38488\\593388613.py:5: DtypeWarning: Columns (63,79,94,96,114,115) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          eventid  iyear  imonth  iday approxdate extended resolution  \\\n",
      "712  200101010005   2001       1     1        NaN       No        NaN   \n",
      "713  200101030006   2001       1     3        NaN       No        NaN   \n",
      "714  200101040001   2001       1     3        NaN       No        NaN   \n",
      "715  200101050006   2001       1     5        NaN       No        NaN   \n",
      "716  200101060005   2001       1     6        NaN       No        NaN   \n",
      "\n",
      "     country country_txt  region  ...  \\\n",
      "712        6     Algeria      10  ...   \n",
      "713       45    Colombia       3  ...   \n",
      "714        6     Algeria      10  ...   \n",
      "715       45    Colombia       3  ...   \n",
      "716      182     Somalia      11  ...   \n",
      "\n",
      "                                              addnotes  \\\n",
      "712                                                NaN   \n",
      "713                                                NaN   \n",
      "714                                                NaN   \n",
      "715  Sources provide different locations for the in...   \n",
      "716  It is unknown whether Ethiopia denies involvem...   \n",
      "\n",
      "                                                scite1  \\\n",
      "712  \"Algeria: Five people reportedly killed by 'te...   \n",
      "713  \"At least 11 murdered in northwest Colombia,\" ...   \n",
      "714  \"Islamists kill 15 more in Algeria: press,\" Ag...   \n",
      "715  \"Thirteen die in second regional massacre,\" As...   \n",
      "716  Osman Hassan, \"Somali Parliament speaker, legi...   \n",
      "\n",
      "                                                scite2  \\\n",
      "712  \"URGENT--- Five people killed in Algeria: offi...   \n",
      "713  \"Eleven killed in northwestern Colombia town,\"...   \n",
      "714  \"Report: 11 soldiers killed in explosion in ea...   \n",
      "715  \"Twelve dead in second Colombia massacre in tw...   \n",
      "716  \"Fate of Somali parliament speaker remains unk...   \n",
      "\n",
      "                                                scite3               dbsource  \\\n",
      "712  \"Latest attacks claim nine lives in violence-t...  UMD Algeria 2010-2012   \n",
      "713  Michael Easterbrook, \"Colombia envoy on missio...        UMD Schmid 2012   \n",
      "714  Patrick Bishop, \"News - International - Algeri...        UMD Schmid 2012   \n",
      "715  \"Twelve Colombian farm workers killed - parami...        UMD Schmid 2012   \n",
      "716  Osman Hassan, \"Parliament speaker comes out of...        UMD Schmid 2012   \n",
      "\n",
      "     INT_LOG INT_IDEO INT_MISC INT_ANY  related  \n",
      "712        0        0        0       0      NaN  \n",
      "713       -9       -9        0      -9      NaN  \n",
      "714        0        0        0       0      NaN  \n",
      "715        0        0        0       0      NaN  \n",
      "716        0        0        0       0      NaN  \n",
      "\n",
      "[5 rows x 135 columns]\n",
      "Number of rows in the filtered dataset: 20425\n",
      "Filtered dataset saved to filtered_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "file_path = 'globalterrorismdb_0522dist_short.csv'  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter the DataFrame to remove rows where iyear <= 2001\n",
    "filtered_df = df[df['iyear'] > 2000]\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame to verify the changes (optional)\n",
    "print(filtered_df.head())\n",
    "\n",
    "filtered_row_count = filtered_df.shape[0]\n",
    "print(f\"Number of rows in the filtered dataset: {filtered_row_count}\")\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file (optional)\n",
    "filtered_file_path = 'filtered_dataset.csv'  # Replace with your file path\n",
    "filtered_df.to_csv(filtered_file_path, index=False)\n",
    "\n",
    "print(f\"Filtered dataset saved to {filtered_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article: September 11 attacks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file d:\\Anaconda\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "{code: Neo.ClientError.Transaction.TransactionHookFailed} {message: You have exceeded the logical size limit of 400000 relationships in your database (attempt to add 40 relationships would reach 400036 relationships). Please consider upgrading to the next tier.}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m documents \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(raw_documents[:\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m     54\u001b[0m graph_documents \u001b[38;5;241m=\u001b[39m llm_transformer\u001b[38;5;241m.\u001b[39mconvert_to_graph_documents(documents)\n\u001b[1;32m---> 55\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_graph_documents(graph_documents, baseEntityLabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, include_source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Process the documents\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(documents):\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\langchain_community\\graphs\\neo4j_graph.py:551\u001b[0m, in \u001b[0;36mNeo4jGraph.add_graph_documents\u001b[1;34m(self, graph_documents, include_source, baseEntityLabel)\u001b[0m\n\u001b[0;32m    546\u001b[0m     document\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m md5(\n\u001b[0;32m    547\u001b[0m         document\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mpage_content\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    548\u001b[0m     )\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# Import nodes\u001b[39;00m\n\u001b[1;32m--> 551\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(\n\u001b[0;32m    552\u001b[0m     node_import_query,\n\u001b[0;32m    553\u001b[0m     {\n\u001b[0;32m    554\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: [el\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m document\u001b[38;5;241m.\u001b[39mnodes],\n\u001b[0;32m    555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m: document\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m,\n\u001b[0;32m    556\u001b[0m     },\n\u001b[0;32m    557\u001b[0m )\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# Import relationships\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(\n\u001b[0;32m    560\u001b[0m     rel_import_query,\n\u001b[0;32m    561\u001b[0m     {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m     },\n\u001b[0;32m    574\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\langchain_community\\graphs\\neo4j_graph.py:392\u001b[0m, in \u001b[0;36mNeo4jGraph.query\u001b[1;34m(self, query, params)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     data \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrun(Query(text\u001b[38;5;241m=\u001b[39mquery, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout), params)\n\u001b[1;32m--> 392\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m [r\u001b[38;5;241m.\u001b[39mdata() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msanitize:\n\u001b[0;32m    394\u001b[0m         json_data \u001b[38;5;241m=\u001b[39m [value_sanitize(el) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m json_data]\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\langchain_community\\graphs\\neo4j_graph.py:392\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     data \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrun(Query(text\u001b[38;5;241m=\u001b[39mquery, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout), params)\n\u001b[1;32m--> 392\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m [r\u001b[38;5;241m.\u001b[39mdata() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msanitize:\n\u001b[0;32m    394\u001b[0m         json_data \u001b[38;5;241m=\u001b[39m [value_sanitize(el) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m json_data]\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\neo4j\\_sync\\work\\result.py:270\u001b[0m, in \u001b[0;36mResult.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_buffer\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mfetch_message()\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discarding:\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discard()\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:178\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m         func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutinefunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__on_error)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:850\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[0;32m    847\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[0;32m    848\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[0;32m    849\u001b[0m )\n\u001b[1;32m--> 850\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_message(tag, fields)\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt5.py:369\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[1;34m(self, tag, fields)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_state_manager\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolt_states\u001b[38;5;241m.\u001b[39mFAILED\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 369\u001b[0m     response\u001b[38;5;241m.\u001b[39mon_failure(summary_metadata \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:245\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[1;34m(self, metadata)\u001b[0m\n\u001b[0;32m    243\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Neo4jError\u001b[38;5;241m.\u001b[39mhydrate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata)\n",
      "\u001b[1;31mClientError\u001b[0m: {code: Neo.ClientError.Transaction.TransactionHookFailed} {message: You have exceeded the logical size limit of 400000 relationships in your database (attempt to add 40 relationships would reach 400036 relationships). Please consider upgrading to the next tier.}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "neo4j_uri = os.getenv(\"NEO4J_URI\")\n",
    "neo4j_username = os.getenv(\"NEO4J_USERNAME\")\n",
    "neo4j_password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"NEO4J_URI\"] = neo4j_uri\n",
    "os.environ[\"NEO4J_USERNAME\"] = neo4j_username\n",
    "os.environ[\"NEO4J_PASSWORD\"] = neo4j_password\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "# List of articles to process\n",
    "articles = [\n",
    "    \"September 11 attacks\",\n",
    "    \"2004 Madrid train bombings\",\n",
    "    \"Boston Marathon bombing\",\n",
    "    \"2008 Christmas massacres\",\n",
    "    \"2008 Mumbai attacks\",\n",
    "    \"2015 Ankara bombings\",\n",
    "    \"Christchurch mosque shootings\",\n",
    "    \"2016 Karrada bombing\",\n",
    "    \"2016 Brussels bombings\",\n",
    "    \"January 2016 Baghdad–Miqdadiyah attacks\"\n",
    "]\n",
    "\n",
    "# Define chunking strategy\n",
    "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "\n",
    "for article in articles:\n",
    "    print(f\"Processing article: {article}\")\n",
    "    \n",
    "    # Read the Wikipedia article\n",
    "    raw_documents = WikipediaLoader(query=article).load()\n",
    "    \n",
    "    # Split the documents into chunks\n",
    "    documents = text_splitter.split_documents(raw_documents[:3])\n",
    "    graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "    graph.add_graph_documents(graph_documents, baseEntityLabel=True, include_source=True)\n",
    "    \n",
    "    # Process the documents\n",
    "    for i, doc in enumerate(documents):\n",
    "        print(f\"Document {i + 1}:\")\n",
    "        print(doc)\n",
    "        print(\"---\")\n",
    "    \n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
